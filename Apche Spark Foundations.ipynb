{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n",
      "Collecting pyarrow==0.14\n",
      "  Downloading pyarrow-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (31.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.6 MB 16.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.0.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting numpy>=1.14\n",
      "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 67.5 MB/s eta 0:00:01\n",
      "\u001b[31mERROR: tensorflow 2.6.0 has requirement numpy~=1.19.2, but you'll have numpy 1.21.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.6.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: sparktspy-nojars 2.0.5.0 has requirement pyspark==3.0.1, but you'll have pyspark 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: itc-utils 0.1.8 has requirement pyarrow>=3.0.0, but you'll have pyarrow 0.14.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: six, numpy, pyarrow\n",
      "Successfully installed numpy-1.21.4 pyarrow-0.14.0 six-1.16.0\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "!pip install pyarrow==0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.0.2\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.12.10, Eclipse OpenJ9 VM, 1.8.0_252\r\n",
      "Branch HEAD\r\n",
      "Compiled by user centos on 2021-02-16T04:53:13Z\r\n",
      "Revision 648457905c4ea7d00e3d88048c63f360045f0714\r\n",
      "Url https://gitbox.apache.org/repos/asf/spark.git\r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the spark session and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.231.93:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://jkg-deployment-6ce2889c-5487-4b9a-88df-ecc8fd24a467-5c46bdftf6m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7effc878ca10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we create an RDD here by calling `sc.parallelize()`\\\n",
    "We create an RDD which has integers from 1 to 10.\n",
    "\n",
    "We then get the number of partitions using the `getNumPartitions()` function and the partitions using the `glom()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "nums = [i for i in range(10)]\n",
    "\n",
    "rdd = sc.parallelize(nums)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see the default partitions done for the RDD. However, we can change that by passing in an optional second argument to the `parallelize` function.\n",
    "Let us now try with 2 and 15 partitions and see how they look like in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism: 2\n",
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 2)\n",
    "\n",
    "print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 15\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 15)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anoter way to partition data is by using the `partitionBy()` function. In this case, the dataset needs to be a tuple with a key/value pair as the default partioner uses a hash for the key to assign elements to a parition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7effc875da10>\n",
      "Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(5, 5), (7, 7), (9, 9), (1, 1), (3, 3)]]\n",
      "Partition: 1 [(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)]\n",
      "Partition: 2 [(5, 5), (7, 7), (9, 9), (1, 1), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums).map(lambda el: (el, el)).partitionBy(2).persist()\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "j = 0\n",
    "for i in rdd.glom().collect():\n",
    "    j += 1\n",
    "    print(\"Partition: \" + str(j) + \" \" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now the elements are distributed differently. A few interesting things happened:\n",
    "\n",
    "```\n",
    "parallelize(nums) - we are transforming Python array into RDD with no partitioning scheme,\n",
    "map(lambda el: (el, el)) - transforming data into the form of a tuple,\n",
    "partitionBy(2) - splitting data into 2 chunks using default hash partitioner\n",
    "```\n",
    "\n",
    "Explicit assignment of partition locations makes the hashing strategy more apparent. The use of the % function assigns it to the correct partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a more practical dataset of transactions. We have 8 transactions from 4 different geographies as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Thomas', 'amount': 30, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "    {'name': 'Pierre', 'amount': 120, 'country': 'France'},\n",
    "    {'name': 'Frank', 'amount': 180, 'country': 'France'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that further analysis will be performed analyzing many similar records within the same country. To optimize network traffic it seems to be a good idea to put records from one country in one node. To meet this requirement, we will need a custom partitioner: Custom partitioner — function returning an integer for given object (tuple key).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3121592\n",
      "9635877\n",
      "1694088\n",
      "2050123\n"
     ]
    }
   ],
   "source": [
    "# Dummy implementation assuring that data for each country is in one partition\n",
    "def country_partitioner(country):\n",
    "    return hash(country)% (10**7+1)\n",
    "    #return portable_hash(country)\n",
    "    \n",
    "\n",
    "# Validate results\n",
    "print(country_partitioner(\"Poland\"))\n",
    "print(country_partitioner(\"Germany\"))\n",
    "print(country_partitioner(\"United Kingdom\"))\n",
    "print(country_partitioner(\"France\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our custom partitioner creates a unique hash for each country name so it can be used to `partitionBy` our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7effc8768590>\n",
      "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})], [('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'}), ('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'})], []]\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "partition: 1\n",
      "[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})]\n",
      "\n",
      "partition: 2\n",
      "[('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})]\n",
      "\n",
      "partition: 3\n",
      "[('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})]\n",
      "\n",
      "partition: 4\n",
      "[('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})]\n",
      "\n",
      "partition: 5\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(5, country_partitioner)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "print(\"\\n--\\n\")\n",
    "for i, j in enumerate(rdd.glom().collect()):\n",
    "    print(\"\\npartition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the partitioning scheme, we can now carry out calculations such as total revenue/sales as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sales(iterator):\n",
    "    yield sum(transaction[1]['amount'] for transaction in iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})], [('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'}), ('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'})], []]\n",
      "Total sales for each partition: [115, 300, 230, 126, 0]\n"
     ]
    }
   ],
   "source": [
    "by_country = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(5, country_partitioner)\n",
    "    \n",
    "print(\"Partitions structure: {}\".format(by_country.glom().collect()))\n",
    "\n",
    "# Sum sales in each partition\n",
    "sum_amounts = by_country \\\n",
    "    .mapPartitions(sum_sales) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Total sales for each partition: {}\".format(sum_amounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ibm/conda/miniconda/lib/python/site-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+\n",
      "|amount|       country|    name|\n",
      "+------+--------------+--------+\n",
      "|   100|United Kingdom|     Bob|\n",
      "|    15|United Kingdom|   James|\n",
      "|    51|        Poland|   Marek|\n",
      "|   200|       Germany|Johannes|\n",
      "|    30|       Germany|  Thomas|\n",
      "|    75|        Poland|    Paul|\n",
      "|   120|        France|  Pierre|\n",
      "|   180|        France|   Frank|\n",
      "+------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7effc8768590>\n",
      "Partitions structure: [[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')], [Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')]\n",
      "partition: 2\n",
      "[Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df.rdd.glom().collect()):\n",
    "    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dataframes we can repartition the dataset by column using the `repartition()` function. The cell below shows how we can partition the dataset by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 'repartition()'\n",
      "Number of partitions: 10\n",
      "Partitioner: None\n",
      "Partitions structure: [[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')], [], [Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')], [], [Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')], [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Repartition by column\n",
    "df2 = df.repartition(10,\"country\")\n",
    "\n",
    "print(\"\\nAfter 'repartition()'\")\n",
    "print(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df2.rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n",
      "partition: 2\n",
      "[]\n",
      "partition: 3\n",
      "[Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')]\n",
      "partition: 4\n",
      "[]\n",
      "partition: 5\n",
      "[Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')]\n",
      "partition: 6\n",
      "[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')]\n",
      "partition: 7\n",
      "[]\n",
      "partition: 8\n",
      "[]\n",
      "partition: 9\n",
      "[]\n",
      "partition: 10\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df2.rdd.glom().collect()):\n",
    "    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
